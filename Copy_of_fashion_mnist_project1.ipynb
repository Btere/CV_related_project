{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMozJJgVSeit2dAw+Hn0BNa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Btere/CV_related_project/blob/main/Copy_of_fashion_mnist_project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJhiRaD-2_f7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "About Dataset:\n",
        "\n",
        "Fashion-MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. The image pixel values are 28x28 grayscale image(1, 28,28), associated with a label from 10 classes.\n",
        "For emphasis, the dataset is an image, that implies we have images and lable(text for the true class- target variable).\n",
        "\n",
        "The dataset can be used to solve supervised learning task: Precisely binary and multiclass problem.\n",
        "\n",
        "We can classify outfit based on the class label we have.\n",
        "\n",
        "When we load an image dataset, there are several preprocessing we can do to the image, before training and evaluting the performance of the model.\n",
        "\n",
        "1. Resizing:\n",
        "\n",
        "Adjust the size of the images to a standard size that the model can process. This is especially important for convolutional neural networks (CNNs) which often require inputs of fixed size.\n",
        "\n",
        "2. Center Crop / Random Crop:\n",
        "\n",
        "Crop the images either centrally or randomly. Central cropping is often used for evaluation, while random cropping can be used for data augmentation during training.\n",
        "\n",
        "3. Normalization(batchNormalization):\n",
        "\n",
        "Normalize the pixel values to have a mean of 0 and a standard deviation of 1. This often helps the model converge faster during training. The mean and standard deviation values are typically computed on the training dataset.\n",
        "\n",
        "4. Conversion to Tensor:\n",
        "\n",
        "Convert the image from a PIL Image or numpy array to a PyTorch tensor.\n",
        "\n",
        "5. Data Augmentation:\n",
        "\n",
        "Apply random transformations to the images to artificially increase the size of the training dataset. Common augmentations include random horizontal flips, rotations, color jitter, etc."
      ],
      "metadata": {
        "id": "N6cNmlPcZ_ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./Data_dir', train=True, download=True, transform=transform)\n",
        "train_images = train_dataset.data\n",
        "target_class = train_dataset.targets\n",
        "\n",
        "\n",
        "test_data = torchvision.datasets.FashionMNIST(root='./Data_dir', train=True, download=True, transform=transform)\n",
        "val_images = test_data.data\n",
        "val_targets = test_data.targets"
      ],
      "metadata": {
        "id": "sFBdVbYS3Y_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Visualization\n",
        "\n",
        "We randomly visualize some of the dataset we have"
      ],
      "metadata": {
        "id": "fwzs60Tl9h-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_image():\n",
        "  \"\"\"We want to visualize the dataset downloaded\"\"\"\n",
        "  col, row = 5, 5\n",
        "  for i in range(1, col * row + 1):\n",
        "    sample_idx = torch.randint(len(train_images), size=(1,)).item()\n",
        "    img = train_images[sample_idx]\n",
        "    figure = plt.figure(figsize=(9, 9))\n",
        "    figure.add_subplot(row, col, i)\n",
        "    plt.axis(\"off\")\n",
        "    # Squeeze is used to remove the single-dimensional entries from the shape of an array.\n",
        "    plt.imshow(img.squeeze(), cmap='gray')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "PDk3pIvckgvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_image()"
      ],
      "metadata": {
        "id": "iQZl2PjWkjHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "aD2DZdx33ZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "cl8em_9X3ZGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = test_data.classes\n",
        "target"
      ],
      "metadata": {
        "id": "Ei8s5yrodlVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further preprocessing anf model training processes\n",
        "\n",
        "1. visualized transformed data\n",
        "2. create data loader\n",
        "3. Build model architecture\n",
        "4. Training and evaluate model\n",
        "5. Apply hyperameter tuning after validation set result"
      ],
      "metadata": {
        "id": "IWFKvCYMubmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_transformed_images(dataset, num_images=5):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        image, label = dataset[i]\n",
        "        image = image.numpy().squeeze()  # Remove batch and channel dimensions\n",
        "        mean = 0.5\n",
        "        std = 0.5\n",
        "        image = image * std + mean  # Denormalize\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.xlabel(f'Label: {label}')\n",
        "    plt.show()\n",
        "\n",
        "# Display transformed images\n",
        "show_transformed_images(train_dataset)"
      ],
      "metadata": {
        "id": "0F1a_GKSdlLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are creating a dataloader, that will load the dataset in batches into the model for training.\n",
        "\n",
        "Why using TensorDataset?\n",
        "\n",
        "TensorDataset is a dataset wrapper in PyTorch that allows you to wrap multiple tensors together and treat them as a single dataset. It is particularly useful when you have your data and labels stored as tensors and want to create a dataset that can be used with PyTorch's DataLoader for batching and shuffling.\n",
        "\n",
        "Assume you have a dataset stored in a single variable and you need to split it into images (features) and labels.\n",
        "\n",
        "Convert your images and labels into PyTorch tensors. Create TensorDataset:\n",
        "Use TensorDataset to combine the image tensors and label tensors.\n",
        "Use DataLoader:\n",
        "\n",
        "Create a DataLoader to handle batching, shuffling, and loading the data efficiently."
      ],
      "metadata": {
        "id": "t2OGm_x8Ez6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def prepare_dataloaders(train_images: torch.tensor, train_targets: torch.tensor, test_images: torch.tensor, test_targets:torch.tensor, batch_size: int = 64, shuffle: bool = True, num_workers=2) -> Tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"Prepare dataset for training in batches\n",
        "  Args:\n",
        "    train_images: Data used as train input.\n",
        "    train_targets: Data used as target label.\n",
        "    test_images: Data used as test input.\n",
        "    test_targets: Data used as test target.\n",
        "    batch_size: Number of samples that will be propagated through the network\n",
        "    shuffle: If true- randomly shuffles data\n",
        "  \"\"\"\n",
        "  train_data = TensorDataset(train_images, train_targets)\n",
        "  train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "  test_data = TensorDataset(test_images, test_targets)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "FTOv51BMdlIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader = prepare_dataloaders(train_images, target_class, val_images, val_targets)\n",
        "#print(train_dataloader, test_dataloader)\n",
        "#Without the dunder method or dataclass, it is the memory address you get, when you want to print out!"
      ],
      "metadata": {
        "id": "EpWRoj-nFoDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the model, we shall be using CNN, Resnet50 and Mobilenet.\n",
        "FOr CNN, we first create the architecture for the model, before forward feed and backward feed.\n",
        "\n",
        "Padding: It control the spatial dimensions of the output feature maps and to prevent loss of information at the borders of the input image.\n",
        "\n",
        "1. Padding of size 1:\n",
        "When you apply padding of size 1, you add 1 row/column of zeros (assuming zero-padding) to the top, bottom, left, and right of the input image.\n",
        "If the original image is  5 X 5 after padding it becomes 7 x 7.\n",
        "\n",
        "2. padding Size 1:\n",
        "\n",
        "When you apply a padding of size 2, you add 2 rows/columns of zeros to the top, bottom, left, and right of the input image.\n",
        "If the original image is 5×5, after padding it becomes  9×9 matrix(tensor).\n",
        "\n",
        "\n",
        "Stride: The stride affect the resolution of the feature map. A stride of 1 preserves the resolution, while a stride of 2 reduces it, making the feature maps smaller and reducing the computational load. it is an hyperparameter\n",
        "\n",
        "Kernel_size: This is the size of the filter applied to the input image to extract features. 2 x 2, 3 X 3 are mostly used, more filter equal lot of data. It is an hyparameter.\n",
        "\n",
        "Pooling layer: It reduces  the spatial dimension of the feature map, they help in making the feature map robust to variation of the input image. We have average and maxpooling to use, this enable us to pick the max pixel value or average pixel value durng convolution.\n",
        "\n",
        "Out_channel: It defines how many different features can be detected. it is an hyperparameter.\n",
        "\n",
        "Stride\n",
        "\n",
        "Strides can be used to control the amount of downsampling at each layer of the network. This helps in reducing the computational load while still capturing important features. Pooling Layers: Strides are often used in pooling layers (e.g., max pooling) to downsample the spatial dimensions, making the representations more abstract and focused on the most salient features.\n",
        "\n",
        "Stride = 1: Minimal reduction in dimensions, essentially moving the filter one pixel at a time.\n",
        "Stride > 1: Greater reduction in dimensions, as the filter skips pixels, covering a larger area with fewer steps.\n",
        "\n",
        "\n",
        "Stride, out_channel, padding are applied, the more the size reduces(height &width), then we get relevant info from the image.\n",
        "\n",
        "Conv2d: This layer applies a 2D convolution operation on the input. The shape of the output feature map depends on the kernel size, stride, and padding.\n",
        "BatchNorm2d: This layer normalizes the feature maps. It doesn’t change the shape of the feature map.\n",
        "MaxPool2d: This layer performs max pooling operation which reduces the spatial dimensions of the feature map depending on the kernel size, stride, and padding.\n",
        "\n",
        "\n",
        "Flattening is the process of converting a multi-dimensional tensor into a 1D tensor. This is typically required before passing the output of convolutional layers to fully connected layers.\n",
        "Why Flatten? Convolutional layers output multi-dimensional tensors (feature maps) while fully connected layers expect 1D tensors. Flattening bridges this gap.\n",
        "\n",
        "Different method to flatten:\n",
        "1. torch.flatten function to flatten the tensor-->x = torch.flatten(x, 1)  # Flatten all dimensions except the batch dimension\n",
        "2. The view method can be used to reshape the tensor. This method is flexible and allows you to reshape the tensor as needed--> x = x.view(x.size(0), -1)  # Flatten all dimensions except the batch dimension.\n",
        "\n",
        "3. You can use the nn.Flatten layer as part of the model definition. This layer can be inserted directly into the nn.Sequential container or used in the forward method --> x = self.flatten(x)  # Using nn.Flatten layer.\n",
        "\n",
        "\n",
        "The fully connected layers in a neural network are designed to combine features learned by the convolutional layers and make predictions. The number of units in these layers can affect:\n",
        "\n",
        "Capacity and Complexity: More units allow the network to learn more complex relationships but may lead to overfitting if not managed properly.\n",
        "Computational Load: More units mean more computations and memory usage, impacting training time and resource requirements.\n",
        "\n",
        "Regularization:\n",
        "\n",
        "Dropout: Apply dropout regularization to prevent overfitting if you use a large number of units.\n",
        "L2 Regularization: Add weight regularization to the dense layers to constrain the model’s capacity and help generalize better.\n",
        "Model Size and Training Resources:\n",
        "\n",
        "Hardware Constraints: Ensure the number of units doesn’t exceed your computational resources or memory limits.\n",
        "Training Time: More units mean longer training times, so balance the number of units with available time and resources.\n"
      ],
      "metadata": {
        "id": "H2fKe_uYHqJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#import torch.nn.functional as F\n",
        "#from torch import Tensor\n",
        "#from torch.utils.data import DataLoader\n",
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "class FashionMnistModel(nn.Module):\n",
        "  def __init__(self, device: torch.Tensor) -> None:\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 2) #\n",
        "    self.bn1 = nn.BatchNorm2d(num_features=32)         #we apply batchnorm, which must be same as the out_channel(extract the feature map)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding = 2)\n",
        "    self.bn2 = nn.BatchNorm2d(num_features=64)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2)\n",
        "    self.bn3 = nn.BatchNorm2d(num_features=128)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=2)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(in_features=128 * 8 * 8, out_features=512)\n",
        "    self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
        "    self.fc3 = nn.Linear(in_features=256, out_features=10) #number of output class:out_feature\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Defines the pipeline of the model.\n",
        "\n",
        "        Args:\n",
        "            x: Model input data.\n",
        "\n",
        "        Returns:\n",
        "            Output generated by the model.\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        print(f'After conv1: {x.shape}')\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        print(f'After pool1: {x.shape}')\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        print(f'After conv1: {x.shape}')\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        print(f'After pool2: {x.shape}')\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        print(f'After conv3: {x.shape}')\n",
        "        x = self.bn3(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.pool3(x)\n",
        "        print(f'After pool3: {x.shape}')\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        print(f'After flatten: {x.shape}')\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train_model(self, model: FashionMnistModel, dataloader: DataLoader, epoch: int = 5) -> Tuple[float, float]:\n",
        "     model.train()\n",
        "    #define loss function\n",
        "     loss_fn = nn.CrossEntropyLoss()\n",
        "     # 4. Optimizer\n",
        "     optimizer = Adam(model.parameters(), lr=0.001)\n",
        "     for epoch in range(epoch):\n",
        "      running_loss: float = 0.0\n",
        "      correct: int = 0\n",
        "      total: int = 0\n",
        "      for images, labels in dataloader:\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #forward pass\n",
        "        prediction = model(images)\n",
        "        #calculate loss\n",
        "        loss = loss_fn(prediction, labels)\n",
        "        #backward pass\n",
        "\n",
        "        loss.backward()\n",
        "        #update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        predicted = prediction.argmax(axis=1)\n",
        "        labels = torch.argmax(labels, dim=1)\n",
        "        correct += (predicted == labels).sum().item() ## Count correct predictions\n",
        "        total += labels.size(0) # # Update the total number of samples\n",
        "      average_loss = running_loss / len(dataloader)\n",
        "      print(f'Epoch [{epoch + 1}/ {model.epoch}], Loss: {average_loss:.4f}')\n",
        "      accuracy = 100 * correct / total\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IOnM0v0CFxxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var = FashionMnistModel(device)\n",
        "#print(var)\n"
      ],
      "metadata": {
        "id": "ypla6mWlKCkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbxsBNwxKCaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important terminologies:\n",
        "\n",
        "Epoch: 1 forward and backward pass for all training samples. Epoch range matters, it is always around 5, 10, 15.\n",
        "\n",
        "Batch_size = Number of training samples for one forward and backward pass\n",
        "\n",
        "Number of iteration = Number of passes, each passes using [batch_size] number of samples.\n",
        "FOr examples, if sample size = 100, batch_size = 20 --> 100/ 20 = 5 iterations for one epoch.\n",
        "If sample size = 10000, batch_size = 64 --> 10000 // 64 =  156 iterations for one epoch."
      ],
      "metadata": {
        "id": "TiWqfNKCAwG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model: FashionMnistModel, save_path: Path, model_name: str) -> None:\n",
        "      \"\"\"We want to save the trained model\n",
        "      Args:\n",
        "        model: Model to be saved.\n",
        "        save_path: Path where the model will be saved.\n",
        "      \"\"\"\n",
        "      Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "      torch.save(model.state_dict(), save_path / model_name)"
      ],
      "metadata": {
        "id": "K5lEUrlMFxhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path) -> nn.Module:\n",
        "      \"\"\"We want to load the trained model\n",
        "      Args:\n",
        "        model: Model to be loaded.\n",
        "        save_path: Path where the model is saved.\n",
        "      \"\"\"\n",
        "      #model = FashionMnistModel()\n",
        "      model = torch.load(model_path)\n",
        "      #return model"
      ],
      "metadata": {
        "id": "wET8KPYQFxd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYi2V5QTAQdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}